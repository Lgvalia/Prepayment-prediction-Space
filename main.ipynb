{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import importlib\n",
    "from import_data import extract_data\n",
    "import preprocessors as pp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     cross_val_score)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MinMaxScaler)\n",
    "from sklearn.metrics import (classification_report,\n",
    "                             confusion_matrix, \n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             auc, \n",
    "                             roc_auc_score, \n",
    "                             ConfusionMatrixDisplay, \n",
    "                             RocCurveDisplay,\n",
    "                             make_scorer)\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from feature_engine.imputation import (MeanMedianImputer,\n",
    "                                       ArbitraryNumberImputer, \n",
    "                                       CategoricalImputer)\n",
    "from feature_engine.selection import (DropFeatures, DropDuplicateFeatures, DropConstantFeatures, RecursiveFeatureAddition, RecursiveFeatureElimination)\n",
    "from feature_engine.encoding import (OneHotEncoder, OrdinalEncoder)\n",
    "from feature_engine.creation import RelativeFeatures\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser, EqualFrequencyDiscretiser\n",
    "from feature_engine.outliers import Winsorizer\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "import optuna\n",
    "\n",
    "import mlflow\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.under_sampling import ClusterCentroids, NearMiss\n",
    "\n",
    "importlib.reload(pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate mlflow tracking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"/home/user/Projects/mlflow/mlruns\")\n",
    "mlflow.set_experiment('experiment_name')\n",
    "mlflow.get_tracking_uri()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mrt_project_features'\n",
    "df = pd.read_csv(f'data/{filename}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA before transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='Load_month')['CustomerId'].count().sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check column dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols_lst = df.select_dtypes(include='O').columns.tolist()\n",
    "num_cols_lst = df.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "bool_cols_lst = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "dt_cols_lst = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find date columns imported as object columns\n",
    "\n",
    "dt_lst = ['date', 'month', 'today']\n",
    "\n",
    "dt_add_lst = []\n",
    "\n",
    "for col in obj_cols_lst:\n",
    "    for dt in dt_lst:\n",
    "        if dt in col.lower():\n",
    "            dt_add_lst.append(col)\n",
    "\n",
    "dt_cols_lst = [] + dt_add_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data leakage by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if dates from collected features are late than load month\n",
    "\n",
    "check_dt_lst = []\n",
    "for col in dt_cols_lst:\n",
    "    if any(pd.to_datetime(df[col]).dt.date > pd.to_datetime(df['Load_month']).dt.date):\n",
    "        check_dt_lst.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print columns with later dates\n",
    "\n",
    "for col in check_dt_lst:\n",
    "    print(col)\n",
    "    print('less than load date:', df.loc[pd.to_datetime(df[col]) < pd.to_datetime(df['Load_month']),[col]].shape[0])\n",
    "    print('more than load date:', df.loc[pd.to_datetime(df[col]) > pd.to_datetime(df['Load_month']),[col]].shape[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_cols_lst].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_target_lst = ['target1','target2','target2']\n",
    "\n",
    "target = 'target1'\n",
    "test_size1 = 0.1\n",
    "test_size2 = 0.3\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(\n",
    "    df.drop(columns=drop_target_lst+['CustomerId','AccountId'], axis=1),\n",
    "    df[target],\n",
    "    test_size=test_size1,\n",
    "    random_state=32\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    test_size=test_size2,\n",
    "    random_state=32\n",
    ")\n",
    "\n",
    "del X_dev\n",
    "del y_dev\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engine's drop constant features are used to remove constant features with a threshold of 0.9\n",
    "\n",
    "drop_transformer = DropConstantFeatures(tol=0.9, missing_values='ignore')\n",
    "drop_transformer.fit(X_train.fillna(0))\n",
    "drop_cols_c = drop_transformer.features_to_drop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engine's drop duplicate features used to remove similar features\n",
    "\n",
    "duplicate_transformer = DropDuplicateFeatures()\n",
    "duplicate_transformer.fit(X_train.drop(columns=drop_cols_c, inplace=True))\n",
    "drop_cols_d = duplicate_transformer.features_to_drop_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most of the columns are hidden due to confidentiality concerns\n",
    "#Empty lists are shown to help understand what preprocessing steps are used\n",
    "\n",
    "obj_to_dt_cols = []\n",
    "\n",
    "lower_obj_cols = []\n",
    "\n",
    "left2_obj_cols = []\n",
    "\n",
    "onehot_obj_cols = []\n",
    "\n",
    "ordinal_obj_cols = []\n",
    "\n",
    "\n",
    "mappings = {\n",
    "    'Column1': {\n",
    "        'attribute1':1,\n",
    "        'attribute2' : 2\n",
    "    },\n",
    "}\n",
    "\n",
    "rel_to_amount = []\n",
    "rel_to_princbal = []\n",
    "rel_to_disbterm = []\n",
    "rel_to_pmt = []\n",
    "\n",
    "Avg_mon_Transfer_Count_lst = []\n",
    "Avg_mon_Transfer_Count_lst_all = []\n",
    "\n",
    "\n",
    "Avg_mon_Transfer_Amount_lst = []\n",
    "Avg_mon_Transfer_Amount_lst_all = []\n",
    "\n",
    "request_count_lst = []\n",
    "request_count_lst_all = []\n",
    "\n",
    "offered_rate_lst = []\n",
    "offered_rate_lst_all = []\n",
    "\n",
    "disbursed_rate_lst = []\n",
    "disbursed_rate_lst_all = []\n",
    "\n",
    "to_date = 'Load_month'\n",
    "\n",
    "date_cols = obj_to_dt_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lgb'\n",
    "version = 'v6'\n",
    "train_period = 'last12M'\n",
    "sel_used = 'Yes'\n",
    "smote_used = 'No'\n",
    "hps_tuned = 'Yes'\n",
    "\n",
    "#these paramters are saved for MLflow run details\n",
    "\n",
    "run_name = f'{target}_{model_name}_{version}_{train_period}_sel{sel_used}'\n",
    "run_descr = f\"\"\"Reitaration of features,\n",
    "Target used in this run: {target} \n",
    "Model used in this run: {model_name},\n",
    "version of the project: {version},\n",
    "period used: {train_period},\n",
    "test+dev size used: {test_size1},\n",
    "feature selections was used: {sel_used},\n",
    "HP were optimized: {hps_tuned},\n",
    "Smote used: {smote_used}\n",
    "Removed features - in_form_inq_workplace_wingslast,'in_form_actual_city_wings','in_form_actual_region_wings', 'fn_region_onboarding_wings'\n",
    "Dropped constant features, \n",
    "\"\"\"\n",
    " \n",
    "#exact hyper parameter values are removed because of confidentiality\n",
    "\n",
    "if model_name == 'lgb':\n",
    "    train_params = {\n",
    "        'bagging_fraction': x,\n",
    "        'bagging_freq': x,\n",
    "        'colsample_bytree': x,\n",
    "        'feature_fraction': x,\n",
    "        'lambda_l1': x,\n",
    "        'lambda_l2': x,\n",
    "        'learning_rate': x,\n",
    "        'max_depth': X,\n",
    "        'min_child_samples': X,\n",
    "        'min_data_in_leaf': X,\n",
    "        'min_gain_to_split': x,\n",
    "        'n_estimators': x,\n",
    "        'num_leaves': x,\n",
    "        'subsample': x\n",
    "        }\n",
    "    model = lgb.LGBMClassifier(**train_params)\n",
    "\n",
    "elif model_name == 'catb': \n",
    "    train_params = {'iterations': x,  \n",
    "                    'learning_rate': x, \n",
    "                    'depth': x, \n",
    "                    'l2_leaf_reg': x, \n",
    "                    'bagging_temperature': x, \n",
    "                    'random_strength': x, \n",
    "                    'loss_function': x, \n",
    "                    'eval_metric': x,\n",
    "                    'random_seed': x}\n",
    "    model = CatBoostClassifier(**train_params)\n",
    "\n",
    "elif model_name == 'xgb':\n",
    "    train_params = {\n",
    "        'n_estimators': x,\n",
    "        'learning_rate': x,\n",
    "        'max_depth': x,\n",
    "        'random_state': x\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**train_params)\n",
    "\n",
    "#here you can find sklearn pipeline\n",
    "#steps which print statuses are added to help with debugging during training. These steps are removed in inference\n",
    "#I moslty use feature engine's built in transformers and my custom transformer (which start with pp.)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('pipeline_start', pp.PrintStatus('pipeline_initiation')),\n",
    "    ('text_to_date_calc', pp.TextToDate([to_date])),\n",
    "    ('text_to_date_calc_status', pp.PrintStatus('text_to_date_calc')),\n",
    "    ('text_to_date', pp.TextToDate(obj_to_dt_cols)),\n",
    "    ('text_to_date_status', pp.PrintStatus('text_to_date')),\n",
    "    ('impute_txt', CategoricalImputer(fill_value='NA')),\n",
    "    ('impute_txt_status', pp.PrintStatus('impute_txt')),  \n",
    "    ('lower_text', pp.LowerText(lower_obj_cols)),\n",
    "    ('lower_text_status', pp.PrintStatus('lower_text')),\n",
    "    ('left_2_text', pp.Left2Text(left2_obj_cols)),\n",
    "    ('left_2_text_status', pp.PrintStatus('left_2_text')),\n",
    "    ('get_dummies', OneHotEncoder(variables=onehot_obj_cols)),\n",
    "    ('get_dummies_status', pp.PrintStatus('get_dummies')),\n",
    "    ('ordinal_encoding', OrdinalEncoder(variables=ordinal_obj_cols, unseen='encode')),\n",
    "    ('ordinal_encoding_status', pp.PrintStatus('ordinal_encoding')),\n",
    "    ('mappers', pp.Mapper(variables=[key for key in mappings], mappings=mappings)),\n",
    "    ('mappers_status', pp.PrintStatus('mappers')),\n",
    "    ('date_to_date', pp.DateToDate(variables=date_cols, mapping=to_date)),\n",
    "    ('date_to_date_status', pp.PrintStatus('date_to_date')),\n",
    "    ('zero_imputer', ArbitraryNumberImputer(arbitrary_number=0)),\n",
    "    ('zero_imputer_status', pp.PrintStatus('zero_imputer')),\n",
    "    ('drop_calc_date', DropFeatures(features_to_drop=[to_date])),\n",
    "    ('drop_calc_date_status', pp.PrintStatus('drop_calc_date')),   \n",
    "    ('div_by_princbal', RelativeFeatures(variables=rel_to_princbal, reference=['PrincipalBalance_snaps'], func=['div'], drop_original=False)),   \n",
    "    ('div_by_princbal_status', pp.PrintStatus('div_by_princbal')),  \n",
    "    ('div_by_amount', RelativeFeatures(variables=rel_to_amount, reference=['Amount_snaps'], func=['div'], drop_original=False)),   \n",
    "    ('div_by_amount_status', pp.PrintStatus('div_by_amount')),\n",
    "    ('div_by_disbterm', RelativeFeatures(variables=rel_to_disbterm, reference=['Disbursed_term_snaps'], func=['div'], drop_original=False)),   \n",
    "    ('div_by_disbterm_status', pp.PrintStatus('div_by_disbterm')), \n",
    "    ('div_by_transfercount', RelativeFeatures(variables=Avg_mon_Transfer_Count_lst, reference=Avg_mon_Transfer_Count_lst_all, func=['div'], drop_original=True, fill_value=0)),\n",
    "    ('div_by_transfercount_status', pp.PrintStatus('div_by_transfercount')), \n",
    "    ('div_by_transferamount', RelativeFeatures(variables=Avg_mon_Transfer_Amount_lst, reference=Avg_mon_Transfer_Amount_lst_all, func=['div'], drop_original=True, fill_value=0)),\n",
    "    ('div_by_transferamount_status', pp.PrintStatus('div_by_transferamount')), \n",
    "    ('div_by_request_count', RelativeFeatures(variables=request_count_lst, reference=request_count_lst_all, func=['div'], drop_original=True, fill_value=0)),\n",
    "    ('div_by_request_count_status', pp.PrintStatus('div_by_request_count')), \n",
    "    ('div_by_offered_rate', RelativeFeatures(variables=offered_rate_lst, reference=offered_rate_lst_all, func=['div'], drop_original=True, fill_value=0)),\n",
    "    ('div_by_offered_rate_status', pp.PrintStatus('div_by_offered_rate')), \n",
    "    ('div_by_disbursed_rate', RelativeFeatures(variables=disbursed_rate_lst, reference=disbursed_rate_lst_all, func=['div'], drop_original=True, fill_value=0)),\n",
    "    ('div_by_disbursed_rate_status', pp.PrintStatus('div_by_disbursed_rate')), \n",
    "    ('outlier_cap', Winsorizer(capping_method='gaussian')),\n",
    "    ('outlier_cap_status', pp.PrintStatus('outlier_cap')), \n",
    "    ('yeojo_transform', YeoJohnsonTransformer() ),    \n",
    "    ('yeojo_transform_status', pp.PrintStatus('yeojo_transform')), \n",
    "    ('discretiser', EqualFrequencyDiscretiser()),\n",
    "    ('discretiser_status', pp.PrintStatus('discretiser')),  \n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('preprocessing', pp.PrintStatus('preprocessing'))\n",
    "])\n",
    "\n",
    "pipeline.steps.append(('model', model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA After transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform data up to some point of pipeline to have numerical variables\n",
    "step_index = list(pipeline.named_steps.keys()).index('date_to_date')\n",
    "X_train_for_eda = pipeline[:step_index+1].fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Correlation with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for column correlated with target variable\n",
    "X_train_for_eda.corrwith(y_train).fillna(0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data distribution up unitl to some point of the pipeline\n",
    "\n",
    "data = X_train_for_eda_outlier_yj\n",
    "columns_for_hist = data.columns.tolist()\n",
    "\n",
    "ncols=3 \n",
    "nrows=math.ceil(len(columns_for_hist)/ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows,ncols=ncols,figsize=[ncols * 5, nrows * 4])\n",
    "for col, ax in zip(columns_for_hist,axes.flatten()):\n",
    "    print(col)\n",
    "    sns.histplot(data=data, x=col, ax=ax)    \n",
    "    # ax.set_title(col)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow tracking is used for experiment tracking\n",
    "# pipeline, its parameters, feature importances and metrics are saved\n",
    "# all of this can be accessed through Mlflow ui to check past pipeline runs\n",
    "\n",
    "with mlflow.start_run(run_name=run_name, description=run_descr) as run:\n",
    "    if model_name == 'lgb':\n",
    "        mlflow.lightgbm.autolog()  \n",
    "\n",
    "    ##fit and log model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # mlflow.log_model(pipeline, \"pipeline\")\n",
    "    mlflow.sklearn.log_model(pipeline, \"pipeline\")\n",
    "    \n",
    "    ##predict labels\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_score = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "\n",
    "    ##classification report for test set\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    mlflow.log_text(report, 'test_classification_report.txt')\n",
    "    print('Test Classification Report')\n",
    "    print(report)\n",
    "\n",
    "    ##classification report for train set\n",
    "    report = classification_report(y_train, y_pred_train)\n",
    "    mlflow.log_text(report, 'train_classification_report.txt')\n",
    "    print('Train Classification Report')\n",
    "    print(report)\n",
    "\n",
    "    ##test f1 \n",
    "    test_precision_label1 = precision_score(y_test, y_pred, pos_label=1)\n",
    "    test_recall_label1 = recall_score(y_test, y_pred, pos_label=1)\n",
    "    test_f1_label1 = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "    ##confusion matrix for test set\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=y_pred)\n",
    "    mlflow.log_figure(cm.figure_, 'test_confusion_matrix.png')\n",
    "\n",
    "    ##log feature importance\n",
    "    feat_imp_df = pd.DataFrame(zip(pipeline[-5].get_feature_names_out(),pipeline[-1].feature_importances_),columns=['Feature','Value']).sort_values('Value', ascending=False)\n",
    "    fig, ax1 = plt.subplots(figsize=(8,15))\n",
    "    sns.barplot(feat_imp_df.head(100), x='Value', y='Feature', ax=ax1)\n",
    "    plt.title('Top 100 Features by Importance')\n",
    "    plt.tight_layout()\n",
    "    mlflow.log_figure(fig, 'Feature_Importances.png')\n",
    "\n",
    "    ##roc auc score for test set\n",
    "    ras = roc_auc_score(y_test, y_score)    \n",
    "\n",
    "    ##log all metrics\n",
    "    metrics = {'test_roc_auc_score':ras,\n",
    "               'test_precision_label1' : test_precision_label1,\n",
    "               'test_recall_label1': test_recall_label1,\n",
    "               'test_f1_label1': test_f1_label1}\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    ##log metrics by months\n",
    "    X_test_pred = X_test.copy()\n",
    "    X_test_pred['actual_label'] = y_test\n",
    "    X_test_pred['predicted_label'] = y_pred\n",
    "    \n",
    "    test_precision_label1_lst = []\n",
    "    test_recall_label1_lst = []\n",
    "    test_f1_label1_lst = []\n",
    "    total_customers_lst = []\n",
    "    y_test_count_lst = []\n",
    "    y_pred_count_lst = []\n",
    "    mon_lst = []\n",
    "    \n",
    "    for mon in X_test_pred['Load_month'].unique():        \n",
    "        filter = X_test['Load_month']==mon\n",
    "        y_test_mon = X_test_pred.loc[filter,'actual_label']\n",
    "        y_pred_mon = X_test_pred.loc[filter,'predicted_label'] \n",
    "    \n",
    "        test_precision_label1_mon = precision_score(y_test_mon, y_pred_mon, pos_label=1)\n",
    "        test_recall_label1_mon = recall_score(y_test_mon, y_pred_mon, pos_label=1)\n",
    "        test_f1_label1_mon = f1_score(y_test_mon, y_pred_mon, pos_label=1)\n",
    "    \n",
    "        total_customers_mon = X_test_pred.loc[filter,'Load_month'].count()\n",
    "        y_test_count_mon = X_test_pred.loc[filter,'actual_label'].sum()\n",
    "        y_pred_count_mon = X_test_pred.loc[filter,'predicted_label'].sum()\n",
    "    \n",
    "        test_precision_label1_lst.append(test_precision_label1_mon)\n",
    "        test_recall_label1_lst.append(test_recall_label1_mon)\n",
    "        test_f1_label1_lst.append(test_f1_label1_mon)\n",
    "        total_customers_lst.append(total_customers_mon)\n",
    "        y_test_count_lst.append(y_test_count_mon)\n",
    "        y_pred_count_lst.append(y_pred_count_mon)\n",
    "        mon_lst.append(mon)\n",
    "    \n",
    "    test_pred_mon_df = pd.DataFrame({'month':mon_lst,\n",
    "                                     'total_customers':total_customers_lst, \n",
    "                                     'y_actual_count':y_test_count_lst,\n",
    "                                     'y_pred_count':y_pred_count_lst,\n",
    "                                     'precision_label1':test_precision_label1_lst,\n",
    "                                     'recall_label1':test_recall_label1_lst,\n",
    "                                     'f1_label1':test_f1_label1_lst\n",
    "                                     })\n",
    "    \n",
    "    del X_test_pred\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "    \n",
    "    test_pred_mon_df_melt = pd.melt(test_pred_mon_df, id_vars='month').sort_values('month')\n",
    "    sns.barplot(data=test_pred_mon_df_melt.loc[test_pred_mon_df_melt['variable'].isin(['total_customers', 'y_actual_count', 'y_pred_count'])],\n",
    "                x='month', y='value', hue='variable', ax=ax1)\n",
    "    ax2 = ax1.twinx()\n",
    "    sns.lineplot(data=test_pred_mon_df_melt.loc[test_pred_mon_df_melt['variable'].isin(['precision_label1', 'recall_label1', 'f1_label1'])],\n",
    "                 x='month', y='value', hue='variable', marker='o', ax=ax2)\n",
    "\n",
    "    mlflow.log_figure(fig, 'metrics_by_months.png')\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SHAP technique for feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I transform X train data from the fit pipeline above\n",
    "X_train_transformed = pipeline[:-1].transform(X_train)\n",
    "\n",
    "shap_values = shap.TreeExplainer(pipeline.named_steps['model']).shap_values(X_train_transformed)\n",
    "shap.summary_plot(shap_values, X_train_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stages for tweaking previous steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some stages repeat steps from above multiple times\n",
    "Not everything is rerun from this stage, it will depend on the changes in above steps and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection technique SelectFromModel of sklearn is used here\n",
    "# this is faster less gready approach\n",
    "# but some other techniques have better results\n",
    "\n",
    "sfm_selector = SelectFromModel(estimator=pipeline, threshold='median' )\n",
    "sfm_selector.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = pipeline.named_steps['drop_calc_date'].get_feature_names_out()\n",
    "sel_features_bool = sfm_selector.get_support()\n",
    "\n",
    "sel_features = [col for col, include in zip(data_columns, sel_features_bool) if include]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engine RFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main disadvantage of the above method is that it removes features based on their importance\n",
    "# that's why I'm using feature engine's Recursive Feature Addition/Elimination\n",
    "# this technique can be aimed at target metric\n",
    "# the features are removed or added depending on the treshold they have on target metric\n",
    "\n",
    "# first I create custom metric, which is most appropriate for the task at hand\n",
    "def custom_score(estimator, X, y):    \n",
    "    y_pred = estimator.predict(X)    \n",
    "    f1 = f1_score(y, y_pred, pos_label=1)\n",
    "    print('label 1 f1 score:', f1)\n",
    "    print(X.shape[1])\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then I ran Recursive feature Addition/elimination CV\n",
    "# I use threshold just above zero to keep almos all features having good effect on traget metric\n",
    "\n",
    "#model from pipeline above is used\n",
    "\n",
    "model = pipeline[-1]\n",
    "\n",
    "# I tried both methods - addition and elimination\n",
    "# despite the fact that RFE had slightly better effect on target metric, I choose RFA because of its speed\n",
    "\n",
    "rfa = RecursiveFeatureAddition(estimator=model, cv=2, scoring=custom_score, threshold=0.0000000001)\n",
    "rfa.fit_transform(X_train_transformed, y_train)\n",
    "\n",
    "# rfe = RecursiveFeatureElimination(estimator=model, cv=2, scoring=custom_score, threshold=0.00001)\n",
    "# rfe.fit_transform(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfa_sel_cols_lst = [col for col in rfa.feature_names_in_ if col not in rfa.features_to_drop_]\n",
    "rfa_sel_cols = []\n",
    "for col in rfa_sel_cols_lst:\n",
    "    rfa_sel_cols.append(int(col[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features then are dropped from data collection step, where possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is transformed up to the modeling stage, so that transformation steps are not repeated\n",
    "\n",
    "X_train_transformed = pipeline[:-1].fit_transform(X_train, y_train)\n",
    "X_val_transformed = pipeline[:-1].transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several resampling techniques were, but one which had better results and faster is kept\n",
    "\n",
    "X_transformed_resampled, y_resampled = SMOTE(sampling_strategy=0.5,k_neighbors=20).fit_resample(X_train_transformed[:,rfa_sel_cols], y_train)\n",
    "# X_transformed_resampled, y_resampled = SMOTEENN().fit_resample(X_train_transformed[:,rfa_sel_cols], y_train)\n",
    "# X_transformed_resampled, y_resampled = ClusterCentroids(random_state=0).fit_resample(X_train_transformed, y_train)\n",
    "# X_transformed_resampled, y_resampled = NearMiss(version=3).fit_resample(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling with resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline[-1].fit(X_transformed_resampled, y_resampled)\n",
    "y_pred = pipeline[-1].predict(X_val_transformed)\n",
    "y_pred_train = pipeline[-1].predict(X_transformed_resampled)\n",
    "\n",
    "##classification report for test set\n",
    "report = classification_report(y_val, y_pred)\n",
    "print('Test Classification Report')\n",
    "print(report)\n",
    "\n",
    "##classification report for train set\n",
    "report = classification_report(y_resampled, y_pred_train)\n",
    "print('Train Classification Report')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna Lightgbm Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can find code for HP tuning\n",
    "# I've used several other methods - gridsearch, randomsearch - which are not shown here\n",
    "# I use optuna library because it can tweak parameters by trying to optimaze target metric\n",
    "# all runs are also tracked using mlflow tracking\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective':'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 1000),\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        # \"bagging_freq\": 1,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 2**10),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"min_gain_to_split\": trial.suggest_float('min_gain_to_split',0.01,0.1),\n",
    "        \"lambda_l1\": trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        \"lambda_l2\": trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),      \n",
    "    \n",
    "    with mlflow.start_run(nested=True) as run:        \n",
    "        model = lgb.LGBMClassifier(**params,random_state=0)\n",
    "        ## Option 1: original data\n",
    "        model.fit(X_train_transformed[:,rfa_sel_cols], y_train, verbose=False)\n",
    "        ## Option 2: oversampled data\n",
    "        # model.fit(X_transformed_resampled, y_resampled, verbose=False)           \n",
    "        mlflow.lightgbm.log_model(lgb_model=model, artifact_path='model.pkl')\n",
    "\n",
    "        predictions = model.predict(X_val_transformed[:,rfa_sel_cols])    \n",
    "        val_f1_score = f1_score(y_val, predictions, pos_label=1) \n",
    "        val_precision_score = precision_score(y_val, predictions, pos_label=1)\n",
    "        val_recall_score = recall_score(y_val, predictions, pos_label=1)\n",
    "\n",
    "        ## Option 1: original data\n",
    "        train_predictions = model.predict(X_train_transformed[:,rfa_sel_cols])    \n",
    "        train_f1_score = f1_score(y_train, train_predictions, pos_label=1) \n",
    "        train_precision_score = precision_score(y_train, train_predictions, pos_label=1)\n",
    "        train_recall_score = recall_score(y_train, train_predictions, pos_label=1)\n",
    "\n",
    "        # Option 2: oversampled data\n",
    "        # train_predictions = model.predict(X_transformed_resampled)    \n",
    "        # train_f1_score = f1_score(y_resampled, train_predictions, pos_label=1) \n",
    "        # train_precision_score = precision_score(y_resampled, train_predictions, pos_label=1)\n",
    "        # train_recall_score = recall_score(y_resampled, train_predictions, pos_label=1)\n",
    "\n",
    "        metrics = {\n",
    "            'val_precision_score': val_precision_score,\n",
    "            'val_recall_score':val_recall_score,\n",
    "            'val_f1_score':val_f1_score,\n",
    "            'train_precision_score': train_precision_score,\n",
    "            'train_recall_score': train_recall_score,\n",
    "            'train_f1_score': train_f1_score\n",
    "        }\n",
    "\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        return val_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all runs can be checked in optuna dashboard in real time\n",
    "\n",
    "study = optuna.create_study(direction='maximize', storage=\"sqlite:///db.sqlite3\", study_name=run_name+'_trial18')\n",
    "with mlflow.start_run(run_name=f'{run_name}_hp_optuna') as run:\n",
    "    study.optimize(objective, n_trials=100)\n",
    "\n",
    "    print('Number of finished trials:', len(study.trials))\n",
    "    print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_trial.params"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

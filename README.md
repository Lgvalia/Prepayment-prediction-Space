# Prepayment-prediction-Space
Repo for Prepayment prediction project at Space International

**Disclaimer:** this project is based on a project I've done at Space International. Thus most of the outputs and inputs are not shown to not expose confidential data. The code is exported and uploaded to GitHub manually at one point of iterations, thus some parts of the code might not work when directly copied. The idea of this repo is to showcase techniques I've used for this end-to-end ML project. 

The aim of this project was to predict and generate list of customers who might make prepayment in the following month. This generated list is then used to communicate with the customers based on their predicted probabilities.

The project starts with data the collection step from SQL database using dbt. Once the data is collected at DWH following steps are done through python and its libraries. Then docker container with FastAPI app is prepared for inference. All of this is orchestrated by airflow (this part is not added to this repo yet)

1. **Data Collection with dbt** - this part takes the biggest hit beacause of the confidentiality, as lots of confidential information could be exposed if whole code was added. Thus I'm showing several key techniques used in dbt. The point of using dbt is to have SQL script as concise and replicable as possible. I use macros so that multiple joinned secondary tables for features will be compiled with macros and project variables. dbt utilizes jinja for loops, which helps to generate hundreds of SQL code with just few lines in dbt. You can check dbt folder for more details.
2. **Training in python** - I use python library Feature engine's transformers to create sklearn pipeline. Most of the transformers in this pipeline are feature engine's built in classes, but I also add several custom classes, which can be found in preprocessors py file. I use MLFlow tracking to have experiment tracking enabled during runs and then compare results of different runs through mlflow ui running on localhost. I check feature importances with SHAP, For feature selection I use feature engine's recursive feature addition/elimination - which in comparison with SKlearn's RFE is adding/reliminating features based on their influence on target metric instead of their model importance. Due to data being inbalanced I use resampling techniques from imblearn. As for the hypeparameter tuning I use Optuna library - which uses TPE and tunes parameters based on custom target metric on validation set. More details can be found in main ipynb file.
3. **Inference - FastAPI and Docker** - FastApi app is prepared for inference and is containerized using docker. Several other helper files are prepared to collect data from DWH and do inference. More details can be found in inference folder

